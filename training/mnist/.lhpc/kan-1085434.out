Starting at Thu Feb  6 10:39:21 PM CET 2025
Running on hosts: cg18
Running on 1 nodes.
Running on 1 processors.
SLURM JobID 1085434 processors.
Node has 32 processors.
Node has  total memory.
Node has 5300 memory per cpu.
Current working directory is /lunarc/nobackup/projects/lu2024-17-38/fu6315ma/training/mnist
Current path is /lunarc/nobackup/projects/lu2024-17-38/fu6315ma/.venv/bin:/sw/pkg/gfxlauncher:/home/fu6315ma/.vscode-server/cli/servers/Stable-cd4ee3b1c348a13bafd8f9ad8060705f6d4b9cba/server/bin/remote-cli:/sw/pkg/gfxlauncher:/home/fu6315ma/.local/bin:/home/fu6315ma/bin:/sw/pkg/gfxlauncher:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:~/bin:.:/sw/pkg/lunarc/bin:/sw/pkg/slurm/local/bin:/opt/thinlinc/bin:/opt/thinlinc/sbin:~/bin:.:/sw/pkg/lunarc/bin:/sw/pkg/slurm/local/bin:~/bin:.:/sw/pkg/lunarc/bin:/sw/pkg/slurm/local/bin
2025-02-06 22:39:21: Training started (directory: /lunarc/nobackup/projects/lu2024-17-38/fu6315ma/training/mnist)\n
cuda
checkpoint directory created: ./model
saving model version 0.0
KAN model has 416704 number of parameters
Train:  torch.Size([60000, 784]) torch.Size([60000, 10])
Test:  torch.Size([10000, 784]) torch.Size([10000, 10])
description:   0%|                                                          | 0/500 [00:00<?, ?it/s]description:   0%|                                                          | 0/500 [00:04<?, ?it/s]
Traceback (most recent call last):
  File "/lunarc/nobackup/projects/lu2024-17-38/fu6315ma/training/mnist/kan_mnist.py", line 23, in <module>
    train_data = ut.train_model(
  File "/lunarc/nobackup/projects/lu2024-17-38/fu6315ma/training/utils.py", line 172, in train_model
    loss = model.fit(dataset, opt=train_data['optimizer'], metrics=(train_acc, test_acc),
  File "/lunarc/nobackup/projects/lu2024-17-38/fu6315ma/pykan_repo/kan/MultKAN.py", line 1555, in fit
    self.update_grid(dataset['train_input'][train_id])
  File "/lunarc/nobackup/projects/lu2024-17-38/fu6315ma/pykan_repo/kan/MultKAN.py", line 717, in update_grid
    self.update_grid_from_samples(x)
  File "/lunarc/nobackup/projects/lu2024-17-38/fu6315ma/pykan_repo/kan/MultKAN.py", line 710, in update_grid_from_samples
    self.get_act(x)
  File "/lunarc/nobackup/projects/lu2024-17-38/fu6315ma/pykan_repo/kan/MultKAN.py", line 2700, in get_act
    self.forward(x)
  File "/lunarc/nobackup/projects/lu2024-17-38/fu6315ma/pykan_repo/kan/MultKAN.py", line 803, in forward
    x_numerical, preacts, postacts_numerical, postspline = self.act_fun[l](x)
  File "/lunarc/nobackup/projects/lu2024-17-38/fu6315ma/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/lunarc/nobackup/projects/lu2024-17-38/fu6315ma/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/lunarc/nobackup/projects/lu2024-17-38/fu6315ma/pykan_repo/kan/KANLayer.py", line 161, in forward
    y = self.scale_base[None,:,:] * base[:,:,None] + self.scale_sp[None,:,:] * y
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.22 GiB. GPU 0 has a total capacity of 39.50 GiB of which 2.84 GiB is free. Including non-PyTorch memory, this process has 36.65 GiB memory in use. Of the allocated memory 35.97 GiB is allocated by PyTorch, and 193.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-02-06 22:39:37: Training done

2025-02-06 22:39:37: END OF SCRIPT REACHED
